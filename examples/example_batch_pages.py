#!/usr/bin/env python3
"""
Example usage of the batch_pages_convert feature.
This example demonstrates how to use the new batch pages processing capability.
"""
import asyncio
import json
import os
import sys

# Add the source directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

async def example_batch_pages_convert():
    """Example demonstrating batch pages conversion."""
    
    # Import the function
    from marker_mcp_server.tools import handle_batch_pages_convert
    
    # Example PDF file path
    pdf_file = "/Users/moatasimfarooque/Downloads/marker-1.7.3/uploads/FarooqueMoatasimReportSeptember2023.pdf"
    
    if not os.path.exists(pdf_file):
        print(f"âŒ Example PDF not found: {pdf_file}")
        print("Please ensure you have a PDF file to test with.")
        return
    
    # Output directory
    output_dir = "/tmp/marker_batch_pages_example"
    os.makedirs(output_dir, exist_ok=True)
    
    print("ğŸ”„ Batch Pages Convert Example")
    print("=" * 40)
    print(f"Input PDF: {pdf_file}")
    print(f"Output Directory: {output_dir}")
    print(f"Configuration: Basic (no LLM)")
    
    # Configuration for batch pages processing
    config = {
        "file_path": pdf_file,
        "output_dir": output_dir,
        "pages_per_chunk": 3,  # Small chunks for demonstration
        "combine_output": True,
        "output_format": "markdown",
        "debug": False,
        "use_llm": False,  # Disable LLM to avoid API key requirements
        "config_json": "examples/basic_batch_pages_config.json"
    }
    
    print(f"\nProcessing with {config['pages_per_chunk']} pages per chunk...")
    
    try:
        # Process the document
        result = await handle_batch_pages_convert(config)
        
        # Display results
        print("\n" + "=" * 50)
        print("ğŸ“Š PROCESSING RESULTS")
        print("=" * 50)
        
        if result.get("success"):
            print("âœ… SUCCESS!")
            print(f"ğŸ“„ Total pages processed: {result.get('total_pages', 'Unknown')}")
            print(f"ğŸ“¦ Total chunks created: {result.get('total_chunks', 'Unknown')}")
            print(f"âœ… Successful chunks: {result.get('successful_chunks', 'Unknown')}")
            
            # Show combined output file if available
            if result.get("combined_output_file"):
                combined_file = result["combined_output_file"]
                print(f"ğŸ“ Combined output: {combined_file}")
                
                if os.path.exists(combined_file):
                    file_size = os.path.getsize(combined_file)
                    print(f"   File size: {file_size:,} bytes")
                    
                    # Show first few lines of the output
                    try:
                        with open(combined_file, 'r', encoding='utf-8') as f:
                            preview = f.read(500)
                            print(f"\nğŸ“‹ Preview (first 500 chars):")
                            print("-" * 30)
                            print(preview)
                            if len(preview) == 500:
                                print("...")
                            print("-" * 30)
                    except Exception as e:
                        print(f"   âš ï¸ Could not preview file: {e}")
            
            # Show chunk details
            chunk_results = result.get("chunk_results", [])
            if chunk_results:
                print(f"\nğŸ“‹ Chunk Processing Details:")
                for chunk in chunk_results:
                    status_icon = "âœ…" if chunk.get("success") else "âŒ"
                    chunk_num = chunk.get("chunk_num", "?")
                    page_range = chunk.get("page_range", "?")
                    print(f"   {status_icon} Chunk {chunk_num}: Pages {page_range}")
                    
                    if not chunk.get("success") and chunk.get("error"):
                        print(f"      âŒ Error: {chunk.get('error')}")
                    elif chunk.get("output_file"):
                        output_file = chunk.get("output_file")
                        if os.path.exists(output_file):
                            size = os.path.getsize(output_file)
                            print(f"      ğŸ“„ Output: {os.path.basename(output_file)} ({size:,} bytes)")
        else:
            print("âŒ FAILED!")
            print(f"Error: {result.get('error', 'Unknown error')}")
            print(f"Message: {result.get('message', 'No message')}")
        
    except Exception as e:
        print(f"âŒ EXCEPTION: {str(e)}")
        import traceback
        traceback.print_exc()
    
    finally:
        print(f"\nğŸ“ Check output directory: {output_dir}")

async def show_feature_info():
    """Display information about the batch pages feature."""
    
    print("ğŸš€ Batch Pages Processing Feature")
    print("=" * 50)
    print("""
This new feature allows you to:

âœ¨ Process large PDF documents efficiently
ğŸ“¦ Split documents into manageable page chunks  
ğŸ”§ Process each chunk independently
ğŸ“ Combine results into a single output
âš¡ Handle memory-intensive documents
ğŸ›¡ï¸  Provide fault tolerance (if one chunk fails, others continue)
ğŸ“Š Track detailed progress information

Key Parameters:
â€¢ file_path: Path to the PDF file
â€¢ pages_per_chunk: Number of pages per chunk (default: 5)
â€¢ combine_output: Whether to combine chunks (default: True)
â€¢ output_format: markdown, json, or html
â€¢ use_llm: Enable LLM for higher quality (requires API keys)

Example configurations are available in:
â€¢ examples/basic_batch_pages_config.json (no LLM)
â€¢ examples/llm_enhanced_config.json (with LLM)
""")

if __name__ == "__main__":
    print("Marker MCP Server - Batch Pages Processing Example")
    print("=" * 60)
    
    # Show feature information
    asyncio.run(show_feature_info())
    
    # Run the example
    print("\nğŸ¬ Running Example...")
    asyncio.run(example_batch_pages_convert())
    
    print("\nâœ¨ Example completed!")
    print(f"ğŸ“š For more information, see: BATCH_PAGES_DOCUMENTATION.md")
